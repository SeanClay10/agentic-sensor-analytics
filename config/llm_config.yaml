# LLM Configuration

llm:
  # Model configuration
  model_name: "llama3.1:8b"
  
  # Ollama server settings
  base_url: "http://localhost:11434"
  
  # Generation parameters
  temperature: 0.1  # Low for consistency
  max_tokens: 4096
  
  # Reliability settings
  timeout: 30  # seconds
  max_retries: 3
  
  # Thresholds
  min_confidence: 0.7

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/llm.log"
  
# Performance settings
performance:
  enable_streaming: true  # Stream responses for better UX
  cache_prompts: false